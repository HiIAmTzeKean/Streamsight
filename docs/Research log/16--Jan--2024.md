---
date: 16--Jan--2024
stage: litreview
description: Data partitioning importance
tags: 
modified: 17--Jan--2024
---
# 16--Jan--2024
## Objective
>[!question] Why does more data degrade the performance of the machine

>[!question] How are current systems splitting the data
## Material read
1. [[sunTakeFreshLook2023]] https://doi.org/10.1145/3539618.3591931
2. https://github.com/swapUniba/ClayRS
3. https://github.com/PreferredAI/cornac
4. https://github.com/recsys-benchmark/DaisyRec-v2.0
5. https://github.com/sisinflab/elliot
6. https://github.com/xue-pai/FuxiCTR
## Reflection
![[Guiding questions]]
### Observation 1
User preference is inferred from the interactions of the target user and other users who display similar interactions. When a global timeline is not observed, there is loss in context information pertaining to the preference that that particular instance.
### Observation 2
From observation 1, when the number of interactions are recent relative to a test point, the test accuracy is likely to be higher.
### Observation 3
The act of observing a global timeline would be to simply
1. order the data in chronological order
2. then incrementally train the model with a sliding window

The number of windows to use for retraining is a hyperparam that can be tuned
### Observation 4
I note that the RS analysed from [here](https://github.com/ACMRecSys/recsys-evaluation-frameworks) favour scikit learn train_test_split method and tend to use Kfold.