\documentclass{article}
\begin{document}

Ng Tze Kean

Project no: SCSE23-0819

Project Title: Development of a RecSys evaluation platform (Part 1)

\section{Abstract}

Recommender systems (RS) are crucial for personalized information retrieval,
but their evaluation methods often neglect the temporal dynamics of user
preferences. This paper addresses this gap by implementing a recommender system
with a focus on incorporating time into the evaluation process. We leverage a
sliding window approach to split the dataset, ensuring the training and
evaluation sets reflect the chronological order of user interactions. This
approach aligns with the core concept presented in a recent research paper,
which argues for a reevaluation of how recommender systems are assessed. Our
implementation utilizes Python libraries and focuses on fairness in algorithm
comparison by allowing models to leverage their preferred data usage patterns.
The evaluation analyzes how the sliding window approach impacts performance
metrics compared to traditional splitting methods. This research contributes to
the field by demonstrating a practical implementation for temporally aware
evaluation in recommender systems, paving the way for more realistic and
effective assessment techniques.

\section{Introduction}

RS have become a ubiquitous technology, driving user engagement and influencing
purchase decisions across various platforms. Their effectiveness hinges on the
ability to accurately predict user preferences and recommend relevant items.
However, traditional evaluation methods for RS often fall short by neglecting
the dynamic nature of user behavior. User preferences can evolve over time,
influenced by trends, seasonality, or even personal experiences. This temporal
dimension plays a crucial role in real-world recommendation scenarios.

Existing open-source recommender system toolkits like DaisyRec [1], Elliot [2],
and RecSysPack [3] offer valuable functionalities for building and evaluating
recommender models. These toolkits provide implementations of various
recommendation algorithms, metrics for performance evaluation, and
functionalities for data pre-processing. However, a key limitation lies in
their approach to temporal evaluation. While they might allow for incorporating
a single global timestamp associated with each interaction, they lack the
ability to implement a global sliding window mechanism.

A global sliding window approach is a crucial concept for temporally aware
evaluation. It involves splitting the dataset into smaller windows that move
chronologically through the entire data sequence. This ensures that the
training data reflects the user behavior up to a specific point in time, while
the evaluation data represents user interactions after that point. This
approach closely mimics real-world scenarios where a recommender system needs
to predict future preferences based on past interactions.

The inability of existing toolkits to implement a global sliding window
evaluation hinders a comprehensive understanding of how recommender systems
perform when considering the temporal dynamics of user behavior. This paper
aims to address this gap by presenting a novel implementation of a recommender
system with a focus on temporally aware evaluation using a sliding window
approach.

This research builds upon the insights from a recent paper [reference the
                original paper here] that argues for a reevaluation of how recommender systems
are assessed. We translate the core concept of the paper into a practical
implementation using Python libraries. Our approach prioritizes fairness in
algorithm comparison by allowing models to leverage their preferred data usage
patterns. Finally, we evaluate the impact of the sliding window approach on the
performance metrics compared to traditional splitting methods.

By demonstrating a practical implementation for temporally aware evaluation,
this research contributes to the development of more realistic and effective
assessment techniques for recommender systems.

\section{Design and implementation}

\subsection*{Design methodology}

\subsection*{Implementation}

what components i used how did i use them what are the software used what
architecture how did i build it why this way

\section{Results}

\section{Discussion}

\section{Conclusion}

Component 1: Restatement of objective and methodological approach of study
Component 2: Review and explanation of key findings Component 3: Implications
of research findings (point out practical applications of study) Component 4:
Limitations of study Component 5: Recommendations for future research

\section{Reference}

\end{document}